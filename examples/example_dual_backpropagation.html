<HEAD>
  <SCRIPT SRC="../ganja.js"></SCRIPT>
</HEAD>
<BODY><SCRIPT>
// A 3 layer convolutional network with AD and Back-Propagation.
// Example from (https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)
// (using automatic differentiation instead, and also optimizing the bias values)

// A 13 element algebra, scalars and 12 duals for derivatives
var basis=[...Array(13)].map((x,i)=>i?'e'+i:'1'),
    Cayley=basis.map((a,i)=>basis.map((b,j)=>i==0?b:j==0?a:"0"));

Algebra({basis,Cayley},()=>{
  // Helper to invert dual numbers with many dual components.
  var inv = x=>x.map((c,i)=>i?-c/(x.s**2):1/c);

  // The initial weights, input and desired output of our network.
  var input  = [.05,.10,1],                                              // input sample.
      WH     = [[.15+1e1,.20+1e2,.35+1e3],[.25+1e4,.30+1e5,.35+1e6]],    // hidden layer weights and bias
      WO     = [[.40+1e7,.45+1e8,.60+1e9],[.50+1e10,.55+1e11,.60+1e12]], // output layer weights and bias
      output = [0.01,0.99];                                              // desired output.

  // The activation function, forward evaluator and error function.
  var logistic = x=>inv(1+this.exp(-x)),
      forward=(sample)=>(WO*[...(WH*sample).map(logistic),1]).map(logistic),
      error = (sample,output)=>.5*(output-sample.slice(0,2))**2;

  // Now train the network.           
  for (var k=0;k<4000;k++) {
     var update = -0.5*error(forward(input), output);               // forward evaluation and error calc
     WO = WO+[[...update.slice(7,10)],[...update.slice(10,13)]];    // backpropagate output layer
     WH = WH+[[...update.slice(1,4)],[...update.slice(4,7)]];       // backpropagate hidden layer
  }

  // Now output the result after 4000 epochs :
  document.body.innerHTML+=`<PRE>Result : ${forward(input).map(x=>x.s)}\nError : ${(-2*update.s).toFixed(6)}
  WH : ${WH.map(x=>x.map(x=>x.s.toFixed(3)))}\n  WO : ${WO.map(x=>x.map(x=>x.s.toFixed(3)))}</PRE>`
});
</SCRIPT></BODY>